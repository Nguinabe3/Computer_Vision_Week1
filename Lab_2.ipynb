{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hqnl0AKVXIA4"},"outputs":[],"source":["import torch\n","from torch import Tensor"]},{"cell_type":"markdown","metadata":{"id":"Y_PmddvQRsvR"},"source":["# Tutorial 1b: Softmax Function"]},{"cell_type":"markdown","metadata":{"id":"g1DV-MS2bxYq"},"source":["**Question:** To have the logistic regressor output probabilities, they need to be processed through a softmax layer. Implement a softmax layer yourself. What numerical issues may arise in this layer? How can you solve them? Use the testing code to confirm you implemented it correctly."]},{"cell_type":"markdown","source":["In the softmax layer, numerical issues can arise due to exponentiating large or very negative numbers.\n","\n","> This can lead to numerical instability and\n",">> - overflow : when numbers become too large to represent\n",">> - underflow : when numbers become too close to zero to represent, which can result in NaN (for example the output of our bad_softmax function) values\n","\n","\n","To solve these numerical issues, several techniques exist but we used the Numerical Stability Techniques in our good_softmax function:\n","\n","> This approach is to normalize the input values before applying the softmax function. This can involve subtracting the maximum value from each input value, which prevents overflow by ensuring that the largest input value becomes zero. The resulting probabilities are mathematically equivalent but more numerically stable."],"metadata":{"id":"MQFd6okZdmgl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTTOJALeRsvR"},"outputs":[],"source":["logits = torch.rand((1, 20)) + 100"]},{"cell_type":"code","source":["logits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"baZ-G7JReAfH","executionInfo":{"status":"ok","timestamp":1714790041432,"user_tz":0,"elapsed":286,"user":{"displayName":"Josue Nguinabe","userId":"15807950764922756039"}},"outputId":"ba6bf55b-6109-4145-8c19-851568d9c67b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[100.6454, 100.2599, 100.5345, 100.7686, 100.6782, 100.7261, 100.9622,\n","         100.0570, 100.1590, 100.5197, 100.2220, 100.3827, 100.8921, 100.6602,\n","         100.4100, 100.7493, 100.8350, 100.5977, 100.9931, 100.5157]])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dj4X2PnOfK9W"},"outputs":[],"source":["def bad_softmax(x: Tensor) -> Tensor:\n","    return torch.exp(x) / torch.sum(torch.exp(logits), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wP3_LslDRsvS","executionInfo":{"status":"ok","timestamp":1714790047663,"user_tz":0,"elapsed":262,"user":{"displayName":"Josue Nguinabe","userId":"15807950764922756039"}},"outputId":"a1bc72b7-4c6c-4c96-db99-155555c7236f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(nan)"]},"metadata":{},"execution_count":5}],"source":["torch.sum(bad_softmax(logits))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QR9Vz3ilRsvS"},"outputs":[],"source":["def good_softmax(x: Tensor) -> Tensor:\n","    ###########################################################################\n","    # TODO: Implement a more stable way to compute softmax                    #\n","    ###########################################################################\n","  z = x - max(x)\n","  softmax = torch.exp(z)/torch.sum(torch.exp(z))\n","  return softmax"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nPSPEUaRsvS","executionInfo":{"status":"ok","timestamp":1714790055663,"user_tz":0,"elapsed":292,"user":{"displayName":"Josue Nguinabe","userId":"15807950764922756039"}},"outputId":"fecd5b52-1fd1-42d6-8626-a09feab25b5d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0000)"]},"metadata":{},"execution_count":7}],"source":["torch.sum(good_softmax(logits))"]},{"cell_type":"markdown","metadata":{"id":"4C_J5S0RScXJ"},"source":["Because of numerical issues like the one you just experiences, PyTorch code typically uses a `LogSoftmax` layer."]},{"cell_type":"markdown","metadata":{"id":"lgStX-ctjIms"},"source":["**Question [optional]:** PyTorch automatically computes the backpropagation gradient of a module for you. However, it can be instructive to derive and implement your own backward function. Try and implement the backward function for your softmax module and confirm that it is correct."]},{"cell_type":"code","source":["import torch\n","\n","class CustomSoftmax(torch.nn.Module):\n","    def forward(self, x):\n","        exp_x = torch.exp(x)\n","        softmax = exp_x / torch.sum(exp_x, dim=1, keepdim=True)\n","        self.softmax = softmax  # Save for backward pass\n","        return softmax\n","\n","    def backward(self, grad_output):\n","        grad_input = []\n","        for i, s in enumerate(self.softmax):\n","            jacobian = torch.diag(s) - torch.outer(s, s)\n","            grad_input.append(torch.matmul(jacobian.t(), grad_output[i]))\n","        return torch.stack(grad_input)"],"metadata":{"id":"2BlhhbQexx7C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Testing the custom softmax module\n","softmax_module = CustomSoftmax()\n","logits = torch.rand(1, 20,requires_grad=True) #+ 100\n","#x = torch.randn(10, 5, requires_grad=True)\n","softmax = softmax_module(logits)\n","loss = softmax.sum()\n","loss.backward()\n","\n","# Checking gradients\n","print(\"Custom softmax gradients:\")\n","print(logits.grad)\n","\n","# Comparing with PyTorch's autograd\n","torch_softmax = torch.nn.functional.softmax(logits, dim=1)\n","loss = torch_softmax.sum()\n","loss.backward()\n","print(\"\\nPyTorch softmax gradients:\")\n","print(logits.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hGXskIQ9x-0m","executionInfo":{"status":"ok","timestamp":1714795786000,"user_tz":0,"elapsed":245,"user":{"displayName":"Josue Nguinabe","userId":"15807950764922756039"}},"outputId":"4f3a134a-b1ca-4357-a4e2-a90a690aed73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Custom softmax gradients:\n","tensor([[-6.4832e-09, -6.0406e-09, -8.2848e-09, -6.0312e-09, -5.5803e-09,\n","         -7.5740e-09, -8.1076e-09, -4.0920e-09, -4.4220e-09, -3.8470e-09,\n","         -3.7530e-09, -6.8119e-09, -6.1010e-09, -4.9001e-09, -4.1363e-09,\n","         -4.6201e-09, -3.7287e-09, -9.3406e-09, -4.7468e-09, -9.3086e-09]])\n","\n","PyTorch softmax gradients:\n","tensor([[-6.4832e-09, -6.0406e-09, -8.2848e-09, -6.0312e-09, -5.5803e-09,\n","         -7.5740e-09, -8.1076e-09, -4.0920e-09, -4.4220e-09, -3.8470e-09,\n","         -3.7530e-09, -6.8119e-09, -6.1010e-09, -4.9001e-09, -4.1363e-09,\n","         -4.6201e-09, -3.7287e-09, -9.3406e-09, -4.7468e-09, -9.3086e-09]])\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1Eq_v8eTk4XAxFXhjL6rk26gwfwvpZYBn","timestamp":1553454632964}]},"kernelspec":{"display_name":"convnet_tutorials","language":"python","name":"convnet_tutorials"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":0}